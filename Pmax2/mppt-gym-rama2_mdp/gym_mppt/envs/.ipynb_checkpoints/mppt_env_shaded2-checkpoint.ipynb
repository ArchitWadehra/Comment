{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3313098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from shaded import Shaded\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "G = [100, 1000]  # read irradiance # Solar radiation in mW / sq.cm\n",
    "T = 25  # read temperature # ojo con kelvin 273\n",
    "SH = [4, 10, 7, 10, 10, 10]  # Shaded modules\n",
    "Mp = 10  # Modules in parallel\n",
    "Ng = [40, 38, 22]  # Parallel-connected series assemblies\n",
    "Iscr_sh = 0.375\n",
    "\n",
    "It=[]\n",
    "Vt=[]\n",
    "Pt=[]\n",
    "Voc=21\n",
    "V0 = list(range(0, Voc*Mp+1))\n",
    "\n",
    "for i in range(len(V0)):\n",
    "    pv = Shaded()\n",
    "    state = pv.data(G, T, i, SH, Ng, Iscr_sh)\n",
    "    I_i=state[0]\n",
    "    V_i=state[1]\n",
    "    P_i=state[2]\n",
    "    It.append(I_i)\n",
    "    Vt.append(V_i)\n",
    "    Pt.append(P_i)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(Vt, It)\n",
    "plt.xlim(0,300)\n",
    "plt.xlabel('Voltage (V)')\n",
    "plt.ylabel('Current (A)')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(Vt, Pt)\n",
    "plt.xlim(0,300)\n",
    "plt.xlabel('Voltage (V)')\n",
    "plt.ylabel('Power (P)')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import gym\n",
    "import gym_mppt\n",
    "from gym_mppt.envs.shaded import Shaded\n",
    "#import numpy as np\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "#from gym_mppt.envs.pvmodel import Panel\n",
    "#from gym_mppt.envs.dc_control import DCcontrol\n",
    "import random\n",
    "\n",
    "class MpptEnvShaded_0(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes': ['human']\n",
    "        # normal = AI plays, renders at 35 fps (i.e. would be used to watch AI play)\n",
    "        # human = Human plays the level to get better acquainted with level, commands, and variables\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.reward_range = (-float('inf'), float('inf'))\n",
    "        # spec = None\n",
    "\n",
    "        self.min_actionValue = -15.0\n",
    "        self.max_actionValue = 15.0\n",
    "\n",
    "        self.max_stateValue = 55000.\n",
    "        self.min_stateValue = -100.\n",
    "\n",
    "        self.state_dim = 3\n",
    "        self.action_dim = 1\n",
    "\n",
    "        self.action_space = spaces.Box(low=self.min_actionValue, high=self.max_actionValue,\n",
    "                                       shape=(self.action_dim,), dtype=np.float32)\n",
    "        \n",
    "        self.observation_space = spaces.Box(low=self.min_stateValue, high=self.max_stateValue,\n",
    "                                       shape=(self.state_dim,), dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.state = np.zeros(self.state_dim) # state = [[V,P,I]]\n",
    "        #self.dt = 0.1 #seconds (it will be used for the reward computing)\n",
    "        self.epsilon = 1. #It is the bandwith for the reward computing\n",
    "\n",
    "        self.Temp = 25.\n",
    "        self.Irr = 1000.\n",
    "\n",
    "        self.steps = 0\n",
    "        self.MaxSteps = 100\n",
    "\n",
    "        #LUIS:\n",
    "        #self.G = [100, 1000]  # read irradiance # Solar radiation in mW / sq.cm\n",
    "        #self.T = 25  # read temperature # ojo con kelvin 273\n",
    "        self.SH = [10, 10, 10, 10, 10, 10]  # Shaded modules\n",
    "        #self.Mp = 10  # Modules in parallel\n",
    "        #self.Ng = [40, 38, 22]  # Parallel-connected series assemblies\n",
    "        #self.Iscr_sh = 0.375\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    ######################################################## Pmax added to function\n",
    "    def step(self, action, Pmax):\n",
    "\n",
    "\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        # leer valor instantaneo de una serie de tiempo\n",
    "        G = self.Irr #read irradiance # Solar radiation in mW / sq.cm\n",
    "        T = self.Temp #read temperature (C) # ojo con kelvin 273\n",
    "\n",
    "        # aca supongo que solo vamos a tener disponibles los ultimos dos valores\n",
    "        '''\n",
    "        sim_length = 2\n",
    "        pv_current = self.state[sim_length, 0]\n",
    "        pv_voltage = self.state[sim_length, 1]\n",
    "        pv_power = self.state[sim_length, 2]\n",
    "        '''\n",
    "        pv_voltage = self.state[0]\n",
    "\n",
    "        #v0 = pv_voltage + action[0][0] # valor anterior de V mas la accion dV\n",
    "        v0 = pv_voltage + action # pa el Colab!\n",
    "        v1 = max(v0,0.)\n",
    "        V = min(v1,212)\n",
    "        #V = v0\n",
    "\n",
    "        # PV and dc-dc models\n",
    "        #pv = Panel()\n",
    "        #self.state = pv.calc_pv(G,T,V)\n",
    "        #I_new, V_new, P_new = pv.calc_pv(G,T,V)  #new_state = [I,V,P]\n",
    "        #print('Desteps pv-calc_pv tengo:','I_new =', I_new, 'V_new = ', V_new, 'P_new =', P_new)\n",
    "\n",
    "        #Luis:\n",
    "        pv = Shaded()\n",
    "        #I_new, V_new, P_new = pv.data(G, T, V, self.SH, self.Ng, self.Iscr_sh)\n",
    "        I_new, V_new, P_new = pv.data(G, T, V,self.SH)\n",
    "        #print ('Temperatura = ', T)\n",
    "        #print ('Irradiancia = ', G)\n",
    "        #print ('SH  = ', self.SH)\n",
    "\n",
    "    \n",
    "\n",
    "        # dc_controller = DCcontrol()\n",
    "        # alpha = action\n",
    "        # V = dc_controller.dcdc(\"buck\", pv_voltage, alpha)\n",
    "\n",
    "        '''\n",
    "        # aca supongo que solo vamos a tener disponibles los ultimos dos valores\n",
    "        dP = self.state[1, 2] - self.state[0, 2] # pv_power(i) - pv_power(i-1)\n",
    "        dV = self.state[1, 1] - self.state[0, 1] # pv_voltage(i) - pv_voltage(i-1)\n",
    "        '''\n",
    "        \n",
    "        dV = V_new - self.state[0] # pv_voltage(i) - pv_voltage(i-1)\n",
    "        dP = P_new - self.state[1] # pv_power(i) - pv_power(i-1)\n",
    "        P = P_new\n",
    "\n",
    "        #print('dv =', dV, 'dP = ', dP, 'P =', P)\n",
    "\n",
    "               \n",
    "\n",
    "        # ojo con el reward por que:\n",
    "        # dP/dV = 0 at MPP\n",
    "        # dP/dV > 0 left of MPP\n",
    "        # dP/dV < 0 right of MPP\n",
    "\n",
    "        # asi esta en el car-on-a-hill continuo\n",
    "        # done = true termina el episodio\n",
    "\n",
    "        ''' Ojo luis, porque con el done asi, el dP/Dv puede ser negativo y da que termina el episodio. Solo comente esto. Lo hago abajo para que quede mas facil...\n",
    "        done = bool(dP/dV <= epsilon)\n",
    "        if done: #(dP/dV >= 0) and (dP/dV < epsilon):\n",
    "            reward = wp * dP\n",
    "        else:\n",
    "            reward = wn * dP\n",
    "\n",
    "        \n",
    "        '''\n",
    "        epsilon = self.epsilon\n",
    "        #done = bool(0<= dP/dV <= epsilon)\n",
    "        #done = bool(np.abs(dP/dV) <= epsilon and P>0)\n",
    "        done = bool(P<=-0.001 or self.steps>=self.MaxSteps)\n",
    "        #print('dP/dV = ', dP/dV, 'P =', P)\n",
    "#         reward = self.reward_function3(dP, P,done) #Poniendo aca una funcion, despues es mas facil para jugar..porque cambiamos el nombre de la funcion y listo...y vamos agregando abajo, tantas como se nos cante...\n",
    "        \n",
    "        ############################################################################\n",
    "        reward = P/Pmax        # Pmax added\n",
    "        ############################################################################\n",
    "    \n",
    "        #The next state is:\n",
    "        #self.state = np.array([[V_new,P_new,I_new]]) #por ahora dejamos I en el estado, pero la podriamos sacar...eventualmete la vamos guardando en una matriz variable del self, por ej: self.currents y chau (esto es por si necesitamos por algo...)\n",
    "        #self.state = np.reshape(np.hstack([V_new,P_new,dV]), (self.state_dim,))\n",
    "        self.state = np.reshape(np.hstack([V_new,P_new,dP]), (self.state_dim,)) \n",
    "        #print('self.state=',self.state,self.state.shape, 'done', done)\n",
    "        #print('EL ESTADO ES', self.state, self.state.shape)\n",
    "        #print('V_new', type(V_new),V_new.shape,'P_new',type(P_new),P_new,'dV',type(dV),dV)\n",
    "\n",
    "        #info = np.array([I_new,T,G,action])\n",
    "\n",
    "        info = {'Corriente': I_new, 'Temperatura':T, 'Irradiancia':G,'Accion':action, 'Steps':self.steps,'v0':v0,'P':P}\n",
    "\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        state_dim = np.size(self.state)\n",
    "        \n",
    "        self.state = np.zeros(state_dim)\n",
    "\n",
    "        self.steps = 0\n",
    "        \n",
    "        a = random.sample([1,2,3,4,5,6,7,8,9,10],1)[0]\n",
    "        b = random.sample([1,2,3,4,5,6,7,8,9,10],1)[0]\n",
    "        c = random.sample([1,2,3,4,5,6,7,8,9,10],1)[0]\n",
    "        #irradiancias = list([100., 200., 300., 400., 500., 600., 700., 800., 900., 1000])\n",
    "        #temperaturas = list([13.5, 15., 17.5, 20., 22.5, 25., 27.5, 30., 32.5, 35])\n",
    "        #self.Temp = 25#random.sample(temperaturas,1)[0] #(Elegir un random de estos) o dejar fija la T y solo variar la irr pa empezar a probar...\n",
    "        #self.Irr = 100#random.sample(irradiancias, 1)[0] #random.sample(irradiancias,1) # [0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0] (Elegir un random de estos)\n",
    "        self.SH = [a, 10, b, 10, c, 10]\n",
    "        #self.SH = [4, 10, 7, 10, 10, 10] #vamos a hacer un entrenamiento simple...luego saco esta linea\n",
    "        return self.state\n",
    "\n",
    "    \n",
    "    def setTempIrr(self,last_state,T,G,SH):\n",
    "        \"\"\"\n",
    "        Esta funcion es para usar unicamente en la simulacion, para cuando le cambiamos la Temp y la Irr\n",
    "        \"\"\"\n",
    "        #self.state = last_state\n",
    "        \n",
    "        self.Temp = T\n",
    "        self.Irr = G\n",
    "        self.SH = SH\n",
    "        \n",
    "        return last_state\n",
    "    \n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "\n",
    "    def take_action(self, action):\n",
    "        pass\n",
    "\n",
    "    def reward_function1(self, dP, P, done):\n",
    "        wp = 0.1\n",
    "        wn = 0.1\n",
    "\n",
    "        if done or dP<0: \n",
    "            r = - 1000\n",
    "        elif dP>=0 and P>0:\n",
    "            r = (wp*P)**2\n",
    " \n",
    "        return r\n",
    "\n",
    "    def reward_function2(self, dP, P, done):\n",
    "        wp = 20.\n",
    "        wn = -4.\n",
    "\n",
    "        if done: #(dP/dV >= 0) and (dP/dV < epsilon):\n",
    "            r = wp * P**2\n",
    "        elif dP > 0 and P>0:\n",
    "            r = wp * dP\n",
    "        elif dP<0 and P>0:\n",
    "            r = wn * dP\n",
    "        elif P <= 0:\n",
    "            r = -20000\n",
    "\n",
    "        return r\n",
    "\n",
    "    def reward_function3(self, dP, P, done):\n",
    "\n",
    "        if P<=0:\n",
    "            r = -1\n",
    "        else:\n",
    "            #r = (P/10000.)**2 - 1.*0\n",
    "            r = P/50000.\n",
    "\n",
    "      \n",
    "        return r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
